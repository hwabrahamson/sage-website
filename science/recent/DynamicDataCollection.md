# Application-agnostic Dynamic Data Collection for AI on the Edge
## Introduction
![Deer Image](../imgs/DDCdeer.png)

Consider a scientific application that uses a SAGE node to capture video in the wild in the hopes of using an AI model to identify the deer and describe what it is doing. In this application, there are large periods of inactivity when the camera captures no deer or other animals, punctuated by short periods of interest when a deer is on camera. In this way, the environment is *dynamic*. However, if SAGE naively runs this job *statically*, without any changes in response to the environment, multiple potential problems arise:
* Because the job relies on video or other high-memory data, the data collected at the edge could exceed the SAGE node's ability to transmit it back.
* Despite generating so much data, most of it is irrelevant or has few features of interest. 
* If this job is being run sparsely in time, it may miss a deer sighting entirely, or to only run the job once during a sighting when it may be possible to run it multiple times.
* On the other hand, the job cannot be run at all times, either because other scientific applications have reserved time on the SAGE node, or because resources like power are scarce and have to be appropriately rationed.

All of these problems are not unique to our hypothetical deer-related application, but apply to a wide variety of tasks submitted to SAGE. Hence, we devise a **framework for dynamic data-collection strategies for AI applications on the edge**. Our framework must have the following two properties:
* **Generality:** There should be no application-specific dependencies in our framework, so that it has broad applicability for future jobs.
* **Edge-Implementability:** The strategies that our framework covers should only have to rely on local decision-making, to ensure prompt reactions to the dynamic environment.

In order to explore our framework, we also create a **simulation**. This simulation is written so that SAGE users interested in a dynamic data collection strategy for their job can freely explore how different strategies might affect their output data, using parameters and features motivated by their specific application. 

## Modelling
Our framework consists of two components: the **environment**, and the **strategy**.
### Environment Model
![environmental state diagram](../imgs/DDCenviron.png)

The environment controls what type of data the strategy can get when it tries to collect a sample. It is modelled using **two states**: an _active_ state in which is it tends to produce more _relevant_ than _irrelevant_ data (in orange above), and a _passive_ state, which produces the opposite (in blue). Essentially, the _active_ state corresponds to an environment in which an event of interest is occurring, while the _passive_ state corresponds to an environment where nothing of note is occurring.

Both the proportions of _relevant_ vs. _irrelevant_ data generated by an _active_ vs. _passive_ environment, as well as the transitions between the _active_ and _passive_ states, are tunable based on the specific application the user is interested in investigating. We implemented the following environmental models:

* **One-State:** A static, one-state environment, used mainly for comparison.

* **Markovian:** A simple 2-state Markov chain. Transition probabilities are tunable parameters.

* **Time Correlations:** A preset version of the Markovian environment. It still has the same two states and generates data stochastically, but instead of having transition probabilities, it instead is given a set of time-ranges during which the environment is _active_, and outside of which it is _passive_.

* **Preset:** A fully deterministic environment that will always produce *relevant* data in the *active* state, and *irrelevant* data in the *passive* state. The environment's states are determined by a user inputted array of labels. Particularly useful if the user already has access to a sample run of their job or a similar job run on SAGE.

### Strategy
![strategy state diagram](../imgs/DDCstrat.png)

The strategy controls how samples are generated. As with the environment, the strategy also has two states, *active* and *passive*. In each state, the strategy can control two parameters:
* **Sample Rate**: How frequently samples are generated, or how often a job is run.
* **Quality Level**: Whether a sample is considered to be high quality or not. This attribute is a little more abstract, but it could represent a variety of changes that make the resulting data more desirable, such as using a higher-end, more expensive AI model instead of a cheaper one, or using a camera's zoom so the video frame has less dead space and is focused on the object of interest.

Hence, a common strategy would be to take low-quality but frequent samples in the passive state, and then take high-quality samples while in the active state.

Also as before, the transitions between states are mutable. We explored two strategies:
* **Responsive:** This strategy transitions to *active* once it senses a piece of *relevant* data. It then transitions down to *passive* after it detects a number of consecutive *irrelevant* data points. The number needed is tunable. Applies neatly to a variety of applications.
* **Waiting Time:** This strategy transitions to *active* once it senses a piece of *relevant* data. It then remains in the *active* state for a number of samples (tunable), at which point it transitions back to the *passive* state if its most recent data point was *irrelevant*, otherwise, it stays *active* for that number of samples again. Useful if the application intends to run multiple AI models at different time scales at the same time.

## Simulation Results
As a proof of concept, we ran the simulation under two conditions. Firstly, we used the Responsive strategy with a Markovian environment with parameters blah blah blah



Next, we devised a simple but real application to have the simulation generate realistic numbers, and then see if those numbers still show a significant gain as in the previous example. We used blah lbah blahl balh
